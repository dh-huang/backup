# 分布式锁

当存在不同的请求对同一个或者同一组资源读取并修改时，如果无法保证其操作的**有序性、原子性**，那么就很有可能出现预期之外的情况，而如何保证操作的有序性和原子性，其实也就是操作的互斥性问题；

<Thinking in Java>一书中写到：

> 基本上所有的并发模式在解决线程冲突问题的时候，都是采用序列化访问共享资源的方案。

在分布式系统环境下，保证操作互斥性的解决方案有：**数据库悲观锁，数据库乐观锁，分布式锁，队列串行化，异步队列分散，Redis原子操作**；其中异步队列分散其实是对分布式锁和队列串行化的一种高并发下的优化；



#### 分布式锁

**顺序一致性**

**MySql悲观锁**



**Redis实现**

https://redis.io/topics/distlock

- 单节点实现：`SET resource_name my_random_value NX PX 30000`，其中**my_random_value是一个在所有clients中的唯一随机值，主要用于unlock时能安全的删除数据**，而未获取到锁的client则一直自旋尝试获取锁

  ```lua
  -- unlock需要比较redis中的值是否和client的值一直，如果不一致则不能删除（client设置的值提前失效，此时可能已经是其他client设置的锁了）
  if redis.call("get",KEYS[1]) == ARGV[1] then
      return redis.call("del",KEYS[1])
  else
      return 0
  end
  ```

  优点：满足高性能，**适用于需求更高效率（Efficiency）的场景，比如避免一个任务被执行多次、避免重复工作，但是就算执行两次可能也没什么太大的问题**

  缺点：

  1. 锁竞争的时候，是通过自旋的方式实现的等待，对cpu的性能消耗较大
  2. 如果某个client获取到了锁，但是在过期时间周期里没有执行完，那么另一个client就有可能获取到锁，互斥性就被打破了（**锁的过期时间设置多长？**）
  3. 主从架构的话，主节点宕掉，但是从节点还未成功同步主节点的锁信息就切换成了主节点，那么另一个client也是能获取到锁的，互斥性也被打破了，即**不是高可用的**（可以在写入锁信息的时候，追加一个强制主从同步的命令）
  4. 基于哨兵的主从架构脑裂，**master单独分区**

- redis cluster实现：**redLock**

  ```
  1. It gets the current time in milliseconds.
  2. It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.
  3. The client computes how much time elapsed in order to acquire the lock, by subtracting from the current time the timestamp obtained in step 1. If and only if the client was able to acquire the lock in the majority of the instances (at least 3), and the total time elapsed to acquire the lock is less than lock validity time, the lock is considered to be acquired.
  4. If the lock was acquired, its validity time is considered to be the initial validity time minus the time elapsed, as computed in step 3.
  5. If the client failed to acquire the lock for some reason (either it was not able to lock N/2+1 instances or the validity time is negative), it will try to unlock all the instances (even the instances it believed it was not able to lock).
  ```

  其中有几点很重要的原则：

  1. **顺序写入：**如果是非顺序写入的话（并发写入），可能导致**活性问题**，所有的client一直获取不了锁
  2. 每个节点创建锁的时候，需要一个过期时间，避免因为某个节点down掉了而影响锁的建立
  3. client计算建立锁的时间要小于锁的超时时间，才算建立成功，最终锁的有效时间是初始有效时间 - 建立锁消耗的时间
  4. **redission的具体实现中还建立了一个`watchdog`，用于定时给锁续期**
  5. 释放锁的时候，要给所有节点发送删除命令
  6. 使用**Redis hash**实现重入，或者`ThreadLocal`（需要考虑本地和Redis过期时间一致性的问题）

  优点：是高可用的，**适用于更多考虑正确性（Correctness）的场景，多个任务执行者只能有一个在执行，否则系统的状态就会被破坏**

  缺点：**RedLock本质上是建立一个同步模型基础上**

  1. 太复杂了，牺牲了一定的性能，而且只能实现单纯的互斥锁
  2. 如果不定时给锁续期的话，在任务执行时间过长的情况下，可能出现锁超时后其他执行者获取到锁，就违背了互斥性了；
  3. 对系统的时钟有比较强的依赖，一旦系统的时钟变得不准确（**系统时间漂移**），算法的安全性也就保证不了了，即**不可靠时钟**；如果有的节点出现向前跳跃，且当前锁只在多数节点上加锁成功，那么会有节点提前锁过期，就有可能出现另一个执行者获取到锁，在前一个client还持有锁的情况下，两个client都认为自己获取到锁了，**分布式锁脑裂**；（其实这个应该是人为保证的）
  4. 网络延迟或者程序停顿过长时间（GC），即**进程Pause**，`watchdog`其实就起不了作用了，这样还是会导致还在使用共享资源的时候，锁因为过期而被释放，导致互斥性被破坏，同样是**分布式锁脑裂**；

  ![](./img/distributed-lock-unsafe-with-process-pause.png)

**Zookeeper实现**

https://zookeeper.apache.org/doc/r3.4.9/recipes.html#sc_recipes_Locks

依赖于Zookeeper的`sequence-ephemeral`以及**`watch`机制**， 以及**`Session`机制**（当建立session时，客户会收到由服务器创建的`session id`和`password`，当自动重连时，客户端都会发送`session id`和`password`给服务端，重建的还是原来的session；session断开之后，Zookeeper会删除相应的临时节点）；为了避免**羊群效应**，每个client不能直接`watch`锁的根节点，而是`watch`排在他前面的节点，类似`AQS`的队列

- 互斥锁

>1. Call **create( )** with a pathname of "_locknode_/lock-" and the *sequence* and *ephemeral* flags set.
>2. Call **getChildren( )** on the lock node *without* setting the watch flag (this is important to avoid the herd effect).
>3. If the pathname created in step **1** has the lowest sequence number suffix, the client has the lock and the client exits the protocol.
>4. The client calls **exists( )** with the watch flag set on the path in the lock directory with the next lowest sequence number.
>5. if **exists( )** returns false, go to step **2**. Otherwise, wait for a notification for the pathname from the previous step before going to step **2**.

- 可重入锁：本地维护了一个**ConcurrentHashMap**
- 读写锁：read lock监听上一个`write-`前缀节点，write lock与互斥锁加锁逻辑大概一致

> **read lock:**
>
> 1. Call **create( )** to create a node with pathname "_locknode_/read-". This is the lock node use later in the protocol. Make sure to set both the *sequence* and *ephemeral* flags.
> 2. Call **getChildren( )** on the lock node *without* setting the *watch* flag - this is important, as it avoids the herd effect.
> 3. If there are no children with a pathname starting with "write-" and having a lower sequence number than the node created in step **1**, the client has the lock and can exit the protocol.
> 4. Otherwise, call **exists( )**, with *watch* flag, set on the node in lock directory with pathname staring with "write-" having the next lowest sequence number.
> 5. If **exists( )** returns *false*, goto step **2**.
> 6. Otherwise, wait for a notification for the pathname from the previous step before going to step **2**
>
> **write lock:**
>
> 1. Call **create( )** to create a node with pathname "_locknode_/write-". This is the lock node spoken of later in the protocol. Make sure to set both *sequence* and *ephemeral* flags.
> 2. Call **getChildren( )** on the lock node *without* setting the *watch* flag - this is important, as it avoids the herd effect.
> 3. If there are no children with a lower sequence number than the node created in step **1**, the client has the lock and the client exits the protocol.
> 4. Call **exists( ),** with *watch* flag set, on the node with the pathname that has the next lowest sequence number.
> 5. If **exists( )** returns *false*, goto step **2**. Otherwise, wait for a notification for the pathname from the previous step before going to step **2**.

优点：

1. 语义清晰实现简单
2. 基于Zookeeper顺序一致性的保证，相比与Redis来说更加安全
3. 不会依赖任何系统的时钟

缺点：

1. 也存在**分布式锁脑裂**的问题，基本上都是因为session超时断开的原因（进程Pause、连接的节点宕机）
2. **ZAB协议，Recovery phae可能会将给client返回失败的request重新apply，所以可能client认为失败了的request，实际上是成功了的，即client认为没有加上锁，但是其实是加锁成功了的，`sequence-ephemeral`节点又是ZK自动生成的后缀，所以可能死锁的，解决方案参考Curator，加一个UUID的前缀，加锁的时候判断是否存在有该前缀的锁了**



**分布式锁脑裂解决**

**即使我们拥有一个完美实现的分布式锁方案（带自动过期），在没有共享资源参与进来提供某种fencing机制的前提下，我们仍然不能够获得足够的安全性**

- **fencing token: **fencing token机制本质上是要求client在每次访问一个共享资源的时候，在执行任何操作之前，先对资源进行某种形式上的**标记**操作，这个标记能够保证持有旧的锁的client请求（延迟到达了）无法操作资源；这种标记操作可以有很多在形式：**递增的epoch number（只有递增的数字才能确保最终收敛到最新的操作结果上）**、**random token（但是无法区分先后顺序）**；**且判断并写入的操作一定要是原子互斥性的**
- **CAS: **如果业务逻辑中的数据本身支持`fencing token`的作用，结合资源服务器的CAS操作，比如扣减库存中的数据库的CAS操作，结合事务的隔离性，失败则回滚



**分布式锁性能优化**

https://blog.csdn.net/qq_42046105/article/details/102577610

**partition**的思想，将资源分为多个partition，分段加锁（还是比如库存100，那么可以分成10个库存加一个锁，如果库存量不够，就加多个partition的锁）；

但是如果真要解决高并发的问题的话，分布式锁并不是一个很好的解决方案，**Redis原子操作 + 分布式队列**



**如果你的应用只需要高性能的分布式锁不要求多高的正确性，那么单节点 Redis 够了；如果你的应用想要保住正确性，那么不建议 Redlock，建议使用一个合适的一致性协调系统，例如 Zookeeper，且保证存在 fencing token。**



**关于分布式锁的问题参考**

[How to do distributed locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)

[基于Redis的分布式锁到底安全吗（上）](https://mp.weixin.qq.com/s/JTsJCDuasgIJ0j95K8Ay8w)

[基于Redis的分布式锁到底安全吗（下）](https://mp.weixin.qq.com/s/4CUe7OpM6y1kQRK8TOC_qQ)

